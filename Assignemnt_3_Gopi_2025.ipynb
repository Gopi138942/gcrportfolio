{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gopi138942/gcrportfolio/blob/main/Assignemnt_3_Gopi_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4709271",
      "metadata": {
        "origin_pos": 1,
        "id": "f4709271"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "Let's think briefly about what we\n",
        "expect from a good predictive model.\n",
        "We want it to peform well on unseen data.\n",
        "Classical generalization theory\n",
        "suggests that to close the gap between\n",
        "train and test performance,\n",
        "we should aim for a simple model.\n",
        "Simplicity can come in the form\n",
        "of a small number of dimensions.\n",
        "We explored this when discussing the\n",
        "monomial basis functions of linear models.\n",
        "Additionally, as we saw when discussing weight decay\n",
        "($\\ell_2$ regularization),\n",
        "the (inverse) norm of the parameters also\n",
        "represents a useful measure of simplicity.\n",
        "Another useful notion of simplicity is smoothness,\n",
        "i.e., that the function should not be sensitive\n",
        "to small changes to its inputs.\n",
        "For instance, when we classify images,\n",
        "we would expect that adding some random noise\n",
        "to the pixels should be mostly harmless.\n",
        "\n",
        "Scientists formalized\n",
        "this idea when he proved that training with input noise\n",
        "is equivalent to Tikhonov regularization.\n",
        "This work drew a clear mathematical connection\n",
        "between the requirement that a function be smooth (and thus simple),\n",
        "and the requirement that it be resilient\n",
        "to perturbations in the input.\n",
        "\n",
        "Then, :citet:`Srivastava.Hinton.Krizhevsky.ea.2014`\n",
        "developed a clever idea for how to apply Bishop's idea\n",
        "to the internal layers of a network, too.\n",
        "Their idea, called *dropout*, involves\n",
        "injecting noise while computing\n",
        "each internal layer during forward propagation,\n",
        "and it has become a standard technique\n",
        "for training neural networks.\n",
        "The method is called *dropout* because we literally\n",
        "*drop out* some neurons during training.\n",
        "Throughout training, on each iteration,\n",
        "standard dropout consists of zeroing out\n",
        "some fraction of the nodes in each layer\n",
        "before calculating the subsequent layer.\n",
        "\n",
        "To be clear, we are imposing\n",
        "our own narrative with the link to Bishop.\n",
        "The original paper on dropout\n",
        "offers intuition through a surprising\n",
        "analogy to sexual reproduction.\n",
        "The authors argue that neural network overfitting\n",
        "is characterized by a state in which\n",
        "each layer relies on a specific\n",
        "pattern of activations in the previous layer,\n",
        "calling this condition *co-adaptation*.\n",
        "Dropout, they claim, breaks up co-adaptation\n",
        "just as sexual reproduction is argued to\n",
        "break up co-adapted genes.\n",
        "While such an justification of this theory is certainly up for debate,\n",
        "the dropout technique itself has proved enduring,\n",
        "and various forms of dropout are implemented\n",
        "in most deep learning libraries.\n",
        "\n",
        "\n",
        "The key challenge is how to inject this noise.\n",
        "One idea is to inject it in an *unbiased* manner\n",
        "so that the expected value of each layer---while fixing\n",
        "the others---equals the value it would have taken absent noise.\n",
        "In Bishop's work, he added Gaussian noise\n",
        "to the inputs to a linear model.\n",
        "At each training iteration, he added noise\n",
        "sampled from a distribution with mean zero\n",
        "$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ to the input $\\mathbf{x}$,\n",
        "yielding a perturbed point $\\mathbf{x}' = \\mathbf{x} + \\epsilon$.\n",
        "In expectation, $E[\\mathbf{x}'] = \\mathbf{x}$.\n",
        "\n",
        "In standard dropout regularization,\n",
        "one zeros out some fraction of the nodes in each layer\n",
        "and then *debiases* each layer by normalizing\n",
        "by the fraction of nodes that were retained (not dropped out).\n",
        "In other words,\n",
        "with *dropout probability* $p$,\n",
        "each intermediate activation $h$ is replaced by\n",
        "a random variable $h'$ as follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h' =\n",
        "\\begin{cases}\n",
        "    0 & \\textrm{ with probability } p \\\\\n",
        "    \\frac{h}{1-p} & \\textrm{ otherwise}\n",
        "\\end{cases}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "By design, the expectation remains unchanged, i.e., $E[h'] = h$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54feb90c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:10.343768Z",
          "iopub.status.busy": "2023-08-18T19:33:10.343431Z",
          "iopub.status.idle": "2023-08-18T19:33:13.572271Z",
          "shell.execute_reply": "2023-08-18T19:33:13.571155Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "54feb90c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a048453",
      "metadata": {
        "origin_pos": 6,
        "id": "4a048453"
      },
      "source": [
        "## Dropout in Practice\n",
        "\n",
        "Recall the MLP with a hidden layer and five hidden units\n",
        "from :numref:`fig_mlp`.\n",
        "When we apply dropout to a hidden layer,\n",
        "zeroing out each hidden unit with probability $p$,\n",
        "the result can be viewed as a network\n",
        "containing only a subset of the original neurons.\n",
        "In :numref:`fig_dropout2`, $h_2$ and $h_5$ are removed.\n",
        "Consequently, the calculation of the outputs\n",
        "no longer depends on $h_2$ or $h_5$\n",
        "and their respective gradient also vanishes\n",
        "when performing backpropagation.\n",
        "In this way, the calculation of the output layer\n",
        "cannot be overly dependent on any\n",
        "one element of $h_1, \\ldots, h_5$.\n",
        "\n",
        "![MLP before and after dropout.](http://d2l.ai/_images/dropout2.svg)\n",
        ":label:`fig_dropout2`\n",
        "\n",
        "Typically, we disable dropout at test time.\n",
        "Given a trained model and a new example,\n",
        "we do not drop out any nodes\n",
        "and thus do not need to normalize.\n",
        "However, there are some exceptions:\n",
        "some researchers use dropout at test time as a heuristic\n",
        "for estimating the *uncertainty* of neural network predictions:\n",
        "if the predictions agree across many different dropout outputs,\n",
        "then we might say that the network is more confident.\n",
        "\n",
        "## Implementation from Scratch\n",
        "\n",
        "To implement the dropout function for a single layer,\n",
        "we must draw as many samples\n",
        "from a Bernoulli (binary) random variable\n",
        "as our layer has dimensions,\n",
        "where the random variable takes value $1$ (keep)\n",
        "with probability $1-p$ and $0$ (drop) with probability $p$.\n",
        "One easy way to implement this is to first draw samples\n",
        "from the uniform distribution $U[0, 1]$.\n",
        "Then we can keep those nodes for which the corresponding\n",
        "sample is greater than $p$, dropping the rest.\n",
        "\n",
        "In the following code, we (**implement a `dropout_layer` function\n",
        "that drops out the elements in the tensor input `X`\n",
        "with probability `dropout`**),\n",
        "rescaling the remainder as described above:\n",
        "dividing the survivors by `1.0-dropout`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dcda4b93",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.577007Z",
          "iopub.status.busy": "2023-08-18T19:33:13.576588Z",
          "iopub.status.idle": "2023-08-18T19:33:13.582474Z",
          "shell.execute_reply": "2023-08-18T19:33:13.581632Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "dcda4b93"
      },
      "outputs": [],
      "source": [
        "def dropout_layer(X, dropout):\n",
        "    assert 0 <= dropout <= 1\n",
        "    if dropout == 1: return torch.zeros_like(X)\n",
        "    mask = (torch.rand(X.shape) > dropout).float()\n",
        "    return mask * X / (1.0 - dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0d5f19",
      "metadata": {
        "origin_pos": 11,
        "id": "7a0d5f19"
      },
      "source": [
        "We can [**test out the `dropout_layer` function on a few examples**].\n",
        "In the following lines of code,\n",
        "we pass our input `X` through the dropout operation,\n",
        "with probabilities 0, 0.5, and 1, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1effb931",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.587061Z",
          "iopub.status.busy": "2023-08-18T19:33:13.586226Z",
          "iopub.status.idle": "2023-08-18T19:33:13.614970Z",
          "shell.execute_reply": "2023-08-18T19:33:13.614053Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "1effb931",
        "outputId": "a65b45e5-2201-4658-f825-861d222760b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dropout_p = 0: tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
            "dropout_p = 0.5: tensor([[ 0.,  2.,  0.,  0.,  8.,  0.,  0.,  0.],\n",
            "        [ 0., 18.,  0.,  0.,  0., 26.,  0., 30.]])\n",
            "dropout_p = 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "X = torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
        "print('dropout_p = 0:', dropout_layer(X, 0))\n",
        "print('dropout_p = 0.5:', dropout_layer(X, 0.5))\n",
        "print('dropout_p = 1:', dropout_layer(X, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "087cdc90",
      "metadata": {
        "origin_pos": 13,
        "id": "087cdc90"
      },
      "source": [
        "### Defining the Model\n",
        "\n",
        "The model below applies dropout to the output\n",
        "of each hidden layer (following the activation function).\n",
        "We can set dropout probabilities for each layer separately.\n",
        "A common choice is to set\n",
        "a lower dropout probability closer to the input layer.\n",
        "We ensure that dropout is only active during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a98d0264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.618877Z",
          "iopub.status.busy": "2023-08-18T19:33:13.618261Z",
          "iopub.status.idle": "2023-08-18T19:33:13.626219Z",
          "shell.execute_reply": "2023-08-18T19:33:13.625088Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "a98d0264"
      },
      "outputs": [],
      "source": [
        "class DropoutMLPScratch(torch.nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super(DropoutMLPScratch, self).__init__()\n",
        "        self.lin1 = nn.LazyLinear(num_hiddens_1)\n",
        "        self.lin2 = nn.LazyLinear(num_hiddens_2)\n",
        "        self.lin3 = nn.LazyLinear(num_outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n",
        "        if self.training:\n",
        "            H1 = dropout_layer(H1, self.dropout_1)\n",
        "        H2 = self.relu(self.lin2(H1))\n",
        "        if self.training:\n",
        "            H2 = dropout_layer(H2, self.dropout_2)\n",
        "        return self.lin3(H2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2793a6d2",
      "metadata": {
        "origin_pos": 18,
        "id": "2793a6d2"
      },
      "source": [
        "### Training\n",
        "\n",
        "Write your code to train the provided network similar to the training of MLPs described early in the lectures on the FashionMNIST Dataset for 10 epochs.\n",
        "\n",
        "https://pytorch.org/vision/0.19/generated/torchvision.datasets.FashionMNIST.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "12f6e01f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:33:13.630286Z",
          "iopub.status.busy": "2023-08-18T19:33:13.629522Z",
          "iopub.status.idle": "2023-08-18T19:34:13.198615Z",
          "shell.execute_reply": "2023-08-18T19:34:13.197238Z"
        },
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "12f6e01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3689690-e4f3-4242-fb19-11e6fd6954dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 22.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 340kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.21MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Epoch 1, Loss: 0.6977, Accuracy: 0.7460\n",
            "Epoch 2, Loss: 0.5111, Accuracy: 0.8157\n",
            "Epoch 3, Loss: 0.4646, Accuracy: 0.8324\n",
            "Epoch 4, Loss: 0.4413, Accuracy: 0.8402\n",
            "Epoch 5, Loss: 0.4221, Accuracy: 0.8462\n",
            "Epoch 6, Loss: 0.4062, Accuracy: 0.8535\n",
            "Epoch 7, Loss: 0.3946, Accuracy: 0.8570\n",
            "Epoch 8, Loss: 0.3868, Accuracy: 0.8601\n",
            "Epoch 9, Loss: 0.3738, Accuracy: 0.8648\n",
            "Epoch 10, Loss: 0.3677, Accuracy: 0.8667\n",
            "Test Accuracy: 0.8621\n"
          ]
        }
      ],
      "source": [
        "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
        "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\n",
        "model = DropoutMLPScratch(**hparams)\n",
        "\n",
        "#write your training and testing code here\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dropout function\n",
        "def dropout_layer(X, dropout):\n",
        "    assert 0 <= dropout <= 1\n",
        "    if dropout == 1:\n",
        "        return torch.zeros_like(X)\n",
        "    mask = (torch.rand(X.shape) > dropout).float()\n",
        "    return mask * X / (1.0 - dropout)\n",
        "\n",
        "# Define DropoutMLPScratch model\n",
        "class DropoutMLPScratch(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super(DropoutMLPScratch, self).__init__()\n",
        "        self.lin1 = nn.LazyLinear(num_hiddens_1)\n",
        "        self.lin2 = nn.LazyLinear(num_hiddens_2)\n",
        "        self.lin3 = nn.LazyLinear(num_outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_1 = dropout_1\n",
        "        self.dropout_2 = dropout_2\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, X):\n",
        "        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n",
        "        if self.training:\n",
        "            H1 = dropout_layer(H1, self.dropout_1)\n",
        "        H2 = self.relu(self.lin2(H1))\n",
        "        if self.training:\n",
        "            H2 = dropout_layer(H2, self.dropout_2)\n",
        "        return self.lin3(H2)\n",
        "\n",
        "# Hyperparameters\n",
        "hparams = {\n",
        "    'num_outputs': 10,       # 10 classes for FashionMNIST\n",
        "    'num_hiddens_1': 256,\n",
        "    'num_hiddens_2': 256,\n",
        "    'dropout_1': 0.5,\n",
        "    'dropout_2': 0.5,\n",
        "    'lr': 0.1\n",
        "}\n",
        "model = DropoutMLPScratch(**hparams)\n",
        "\n",
        "# Load FashionMNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=model.lr)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, '\n",
        "              f'Accuracy: {correct / total:.4f}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.numpy())\n",
        "            all_labels.extend(y_batch.numpy())\n",
        "    return accuracy_score(all_labels, all_preds)\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "train(model, train_loader, epochs=10)\n",
        "\n",
        "# Test the model\n",
        "test_accuracy = evaluate(model, test_loader)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101c89c1",
      "metadata": {
        "origin_pos": 20,
        "id": "101c89c1"
      },
      "source": [
        "## Higher Level Implementation\n",
        "\n",
        "With high-level APIs, all we need to do is add a `Dropout` layer\n",
        "after each fully connected layer,\n",
        "passing in the dropout probability\n",
        "as the only argument to its constructor.\n",
        "During training, the `Dropout` layer will randomly\n",
        "drop out outputs of the previous layer\n",
        "(or equivalently, the inputs to the subsequent layer)\n",
        "according to the specified dropout probability.\n",
        "When not in training mode,\n",
        "the `Dropout` layer simply passes the data through during testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "224bafde",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:34:13.202812Z",
          "iopub.status.busy": "2023-08-18T19:34:13.202080Z",
          "iopub.status.idle": "2023-08-18T19:34:13.208377Z",
          "shell.execute_reply": "2023-08-18T19:34:13.207307Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "224bafde"
      },
      "outputs": [],
      "source": [
        "class DropoutMLP(torch.nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
        "                 dropout_1, dropout_2, lr):\n",
        "        super(DropoutMLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(),\n",
        "            nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(),\n",
        "            nn.Dropout(dropout_2), nn.LazyLinear(num_outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877d8ec2",
      "metadata": {
        "origin_pos": 27,
        "id": "877d8ec2"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Next, write your code to train the given model on the FashionMNIST Dataset for 10 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d9e0ea94",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:34:13.212381Z",
          "iopub.status.busy": "2023-08-18T19:34:13.211782Z",
          "iopub.status.idle": "2023-08-18T19:35:25.030389Z",
          "shell.execute_reply": "2023-08-18T19:35:25.029011Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "d9e0ea94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334de292-e528-4b15-b7b6-16f01238c20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6913, Accuracy: 0.7480\n",
            "Epoch 2, Loss: 0.5038, Accuracy: 0.8193\n",
            "Epoch 3, Loss: 0.4648, Accuracy: 0.8328\n",
            "Epoch 4, Loss: 0.4407, Accuracy: 0.8397\n",
            "Epoch 5, Loss: 0.4204, Accuracy: 0.8482\n",
            "Epoch 6, Loss: 0.4100, Accuracy: 0.8517\n",
            "Epoch 7, Loss: 0.3925, Accuracy: 0.8583\n",
            "Epoch 8, Loss: 0.3838, Accuracy: 0.8606\n",
            "Epoch 9, Loss: 0.3761, Accuracy: 0.8648\n",
            "Epoch 10, Loss: 0.3672, Accuracy: 0.8670\n",
            "Test Accuracy: 0.8691\n"
          ]
        }
      ],
      "source": [
        "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
        "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\n",
        "model = DropoutMLP(**hparams)\n",
        "\n",
        "#write your training and testing code here\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define DropoutMLP model\n",
        "class DropoutMLP(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2, dropout_1, dropout_2, lr):\n",
        "        super(DropoutMLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(num_hiddens_1), nn.ReLU(),\n",
        "            nn.Dropout(dropout_1),\n",
        "            nn.LazyLinear(num_hiddens_2), nn.ReLU(),\n",
        "            nn.Dropout(dropout_2),\n",
        "            nn.LazyLinear(num_outputs)\n",
        "        )\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "# Hyperparameters\n",
        "hparams = {\n",
        "    'num_outputs': 10,  # 10 classes for FashionMNIST\n",
        "    'num_hiddens_1': 256,\n",
        "    'num_hiddens_2': 256,\n",
        "    'dropout_1': 0.5,\n",
        "    'dropout_2': 0.5,\n",
        "    'lr': 0.1\n",
        "}\n",
        "model = DropoutMLP(**hparams)\n",
        "\n",
        "# Load FashionMNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=hparams['lr'])\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, '\n",
        "              f'Accuracy: {correct / total:.4f}')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    return accuracy\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "train(model, train_loader, epochs=10)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = evaluate(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54985a45",
      "metadata": {
        "origin_pos": 29,
        "id": "54985a45"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Beyond controlling the number of dimensions and the size of the weight vector, dropout is yet another tool for avoiding overfitting. Often tools are used jointly.\n",
        "Note that dropout is\n",
        "used only during training:\n",
        "it replaces an activation $h$ with a random variable with expected value $h$.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. What happens if you change the dropout probabilities for the first and second layers? In particular, what happens if you switch the ones for both layers? Design an experiment to answer these questions, describe your results quantitatively, and summarize the qualitative takeaways.\n",
        "1. Increase the number of epochs to 50 and compare the results.\n",
        "1. Why is dropout not typically used at test time?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\"\"\"What happens if you change the dropout probabilities for the first and second layers? In particular, what happens if you switch the ones for both layers? Design an experiment to answer these questions, describe your results quantitatively, and summarize the qualitative takeaways.\"\"\"\"\n",
        "\n",
        "def train_and_evaluate(dropout_1, dropout_2, epochs=10):\n",
        "    # Initialize the model with specified dropout values\n",
        "    model = DropoutMLP(\n",
        "        num_outputs=10, num_hiddens_1=256, num_hiddens_2=256,\n",
        "        dropout_1=dropout_1, dropout_2=dropout_2, lr=0.1\n",
        "    )\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, '\n",
        "              f'Training Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "    return test_accuracy\n",
        "\n",
        "# Run experiments with different dropout configurations\n",
        "print(\"Experiment 1: Dropout 1 = 0.2, Dropout 2 = 0.5\")\n",
        "test_accuracy_1 = train_and_evaluate(0.2, 0.5)\n",
        "\n",
        "print(\"\\nExperiment 2: Dropout 1 = 0.5, Dropout 2 = 0.2\")\n",
        "test_accuracy_2 = train_and_evaluate(0.5, 0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfCXOmHZKpNu",
        "outputId": "4f5397f0-9013-43e0-b399-a04a3a087b80"
      },
      "id": "dfCXOmHZKpNu",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 1: Dropout 1 = 0.2, Dropout 2 = 0.5\n",
            "Epoch 1, Loss: 0.6337, Training Accuracy: 0.7690\n",
            "Epoch 2, Loss: 0.4537, Training Accuracy: 0.8364\n",
            "Epoch 3, Loss: 0.4081, Training Accuracy: 0.8519\n",
            "Epoch 4, Loss: 0.3833, Training Accuracy: 0.8613\n",
            "Epoch 5, Loss: 0.3599, Training Accuracy: 0.8690\n",
            "Epoch 6, Loss: 0.3487, Training Accuracy: 0.8736\n",
            "Epoch 7, Loss: 0.3315, Training Accuracy: 0.8791\n",
            "Epoch 8, Loss: 0.3229, Training Accuracy: 0.8827\n",
            "Epoch 9, Loss: 0.3112, Training Accuracy: 0.8868\n",
            "Epoch 10, Loss: 0.3016, Training Accuracy: 0.8888\n",
            "Test Accuracy: 0.8663\n",
            "\n",
            "Experiment 2: Dropout 1 = 0.5, Dropout 2 = 0.2\n",
            "Epoch 1, Loss: 0.6577, Training Accuracy: 0.7587\n",
            "Epoch 2, Loss: 0.4800, Training Accuracy: 0.8251\n",
            "Epoch 3, Loss: 0.4395, Training Accuracy: 0.8399\n",
            "Epoch 4, Loss: 0.4158, Training Accuracy: 0.8505\n",
            "Epoch 5, Loss: 0.3975, Training Accuracy: 0.8557\n",
            "Epoch 6, Loss: 0.3806, Training Accuracy: 0.8617\n",
            "Epoch 7, Loss: 0.3723, Training Accuracy: 0.8636\n",
            "Epoch 8, Loss: 0.3611, Training Accuracy: 0.8679\n",
            "Epoch 9, Loss: 0.3528, Training Accuracy: 0.8698\n",
            "Epoch 10, Loss: 0.3458, Training Accuracy: 0.8726\n",
            "Test Accuracy: 0.8658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define DropoutMLP model\n",
        "class DropoutMLP(nn.Module):\n",
        "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2, dropout_1, dropout_2, lr):\n",
        "        super(DropoutMLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(num_hiddens_1), nn.ReLU(),\n",
        "            nn.Dropout(dropout_1),\n",
        "            nn.LazyLinear(num_hiddens_2), nn.ReLU(),\n",
        "            nn.Dropout(dropout_2),\n",
        "            nn.LazyLinear(num_outputs)\n",
        "        )\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "# Hyperparameters\n",
        "hparams = {\n",
        "    'num_outputs': 10,  # 10 classes for FashionMNIST\n",
        "    'num_hiddens_1': 256,\n",
        "    'num_hiddens_2': 256,\n",
        "    'dropout_1': 0.5,\n",
        "    'dropout_2': 0.5,\n",
        "    'lr': 0.1\n",
        "}\n",
        "model = DropoutMLP(**hparams)\n",
        "\n",
        "# Load FashionMNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=hparams['lr'])\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, epochs=50):\n",
        "    print(f\"{'=' * 20} Training Starts {'=' * 20}\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        print(f\"[Epoch {epoch + 1:02d}/{epochs}] \"\n",
        "              f\"Loss: {total_loss / len(train_loader):.4f} | \"\n",
        "              f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "    print(f\"{'=' * 20} Training Complete {'=' * 20}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader):\n",
        "    print(f\"\\n{'=' * 20} Evaluation Starts {'=' * 20}\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"{'=' * 20} Evaluation Complete {'=' * 20}\")\n",
        "    return accuracy\n",
        "\n",
        "# Train the model for 50 epochs\n",
        "print(\"Training for 50 epochs...\\n\")\n",
        "train(model, train_loader, epochs=50)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"\\nEvaluating model after 50 epochs...\")\n",
        "test_accuracy_50 = evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6EPMGsBK5QO",
        "outputId": "be452461-f44a-4cbe-89da-69e52f33119d"
      },
      "id": "S6EPMGsBK5QO",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50 epochs...\n",
            "\n",
            "==================== Training Starts ====================\n",
            "[Epoch 01/50] Loss: 0.6913 | Training Accuracy: 74.78%\n",
            "[Epoch 02/50] Loss: 0.5070 | Training Accuracy: 81.73%\n",
            "[Epoch 03/50] Loss: 0.4636 | Training Accuracy: 83.38%\n",
            "[Epoch 04/50] Loss: 0.4386 | Training Accuracy: 84.37%\n",
            "[Epoch 05/50] Loss: 0.4209 | Training Accuracy: 84.92%\n",
            "[Epoch 06/50] Loss: 0.4066 | Training Accuracy: 85.38%\n",
            "[Epoch 07/50] Loss: 0.3927 | Training Accuracy: 85.84%\n",
            "[Epoch 08/50] Loss: 0.3815 | Training Accuracy: 86.33%\n",
            "[Epoch 09/50] Loss: 0.3758 | Training Accuracy: 86.30%\n",
            "[Epoch 10/50] Loss: 0.3666 | Training Accuracy: 86.73%\n",
            "[Epoch 11/50] Loss: 0.3598 | Training Accuracy: 86.97%\n",
            "[Epoch 12/50] Loss: 0.3543 | Training Accuracy: 87.11%\n",
            "[Epoch 13/50] Loss: 0.3448 | Training Accuracy: 87.39%\n",
            "[Epoch 14/50] Loss: 0.3426 | Training Accuracy: 87.38%\n",
            "[Epoch 15/50] Loss: 0.3376 | Training Accuracy: 87.64%\n",
            "[Epoch 16/50] Loss: 0.3333 | Training Accuracy: 87.78%\n",
            "[Epoch 17/50] Loss: 0.3267 | Training Accuracy: 88.07%\n",
            "[Epoch 18/50] Loss: 0.3256 | Training Accuracy: 88.19%\n",
            "[Epoch 19/50] Loss: 0.3199 | Training Accuracy: 88.29%\n",
            "[Epoch 20/50] Loss: 0.3179 | Training Accuracy: 88.38%\n",
            "[Epoch 21/50] Loss: 0.3149 | Training Accuracy: 88.37%\n",
            "[Epoch 22/50] Loss: 0.3116 | Training Accuracy: 88.60%\n",
            "[Epoch 23/50] Loss: 0.3094 | Training Accuracy: 88.52%\n",
            "[Epoch 24/50] Loss: 0.3052 | Training Accuracy: 88.76%\n",
            "[Epoch 25/50] Loss: 0.3005 | Training Accuracy: 88.87%\n",
            "[Epoch 26/50] Loss: 0.2999 | Training Accuracy: 88.95%\n",
            "[Epoch 27/50] Loss: 0.2977 | Training Accuracy: 88.98%\n",
            "[Epoch 28/50] Loss: 0.2947 | Training Accuracy: 89.15%\n",
            "[Epoch 29/50] Loss: 0.2917 | Training Accuracy: 89.30%\n",
            "[Epoch 30/50] Loss: 0.2890 | Training Accuracy: 89.26%\n",
            "[Epoch 31/50] Loss: 0.2879 | Training Accuracy: 89.28%\n",
            "[Epoch 32/50] Loss: 0.2864 | Training Accuracy: 89.33%\n",
            "[Epoch 33/50] Loss: 0.2842 | Training Accuracy: 89.50%\n",
            "[Epoch 34/50] Loss: 0.2820 | Training Accuracy: 89.54%\n",
            "[Epoch 35/50] Loss: 0.2770 | Training Accuracy: 89.66%\n",
            "[Epoch 36/50] Loss: 0.2787 | Training Accuracy: 89.75%\n",
            "[Epoch 37/50] Loss: 0.2777 | Training Accuracy: 89.69%\n",
            "[Epoch 38/50] Loss: 0.2735 | Training Accuracy: 89.83%\n",
            "[Epoch 39/50] Loss: 0.2733 | Training Accuracy: 89.84%\n",
            "[Epoch 40/50] Loss: 0.2684 | Training Accuracy: 89.99%\n",
            "[Epoch 41/50] Loss: 0.2657 | Training Accuracy: 90.06%\n",
            "[Epoch 42/50] Loss: 0.2671 | Training Accuracy: 89.94%\n",
            "[Epoch 43/50] Loss: 0.2645 | Training Accuracy: 90.12%\n",
            "[Epoch 44/50] Loss: 0.2651 | Training Accuracy: 90.03%\n",
            "[Epoch 45/50] Loss: 0.2605 | Training Accuracy: 90.14%\n",
            "[Epoch 46/50] Loss: 0.2650 | Training Accuracy: 90.15%\n",
            "[Epoch 47/50] Loss: 0.2621 | Training Accuracy: 90.27%\n",
            "[Epoch 48/50] Loss: 0.2646 | Training Accuracy: 90.09%\n",
            "[Epoch 49/50] Loss: 0.2590 | Training Accuracy: 90.43%\n",
            "[Epoch 50/50] Loss: 0.2578 | Training Accuracy: 90.31%\n",
            "==================== Training Complete ====================\n",
            "\n",
            "Evaluating model after 50 epochs...\n",
            "\n",
            "==================== Evaluation Starts ====================\n",
            "Test Accuracy: 88.94%\n",
            "==================== Evaluation Complete ====================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}